{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31962be9-094f-4ec5-a908-cd0c0da83d20",
   "metadata": {},
   "source": [
    "## Ravita Kartawinata\n",
    "https://github.com/Pii-USD/509-tm-token-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/users/rkartawi/Desktop/Ravita/MSADS/509/ads509-tm-scrape/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = os.path.join(data_location, \"twitter/\")\n",
    "lyrics_folder = os.path.join(data_location, \"lyrics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :      \n",
    "    # Given a list of tokens, print number of tokens, number of unique tokens, and number of characters\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens_tot = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    num_characters = sum(\n",
    "        len(token) for token in tokens)    \n",
    "    \n",
    "    # Get lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "    # and num_tokens,  most common tokens.\n",
    "    lexical_diversity = \n",
    "        num_unique_tokens / num_tokens_tot if num_tokens_tot > 0 else 0.0\n",
    "    token_counts = Counter(tokens)\n",
    "    most_common_tokens = token_counts.most_common(num_tokens)\n",
    "\n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens_tot} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")  \n",
    "        print(f\"The {num_tokens} most common tokens are:\")\n",
    "        for token, count in most_common_tokens:\n",
    "            print(f\"{token}: {count}\")\n",
    "        \n",
    "    # Return a list with the number of tokens, number of unique tokens, lexical diversity, and number of characters, most common tokens\n",
    "    return [num_tokens_tot, num_unique_tokens, lexical_diversity, num_characters, most_common_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "# Where text will be the top word and  next text word will be here then follow by text example here List the top five text\n",
    "# Word from text example here is going to be first five word.split() \n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: An assertion statement is a tool to test if a condition of the code is true. If the condition is True, the program continues running; \n",
    "if it evaluates to False, an AssertionError is raised, and the optional error message is displayed. \n",
    "\n",
    "Some of benefits of assertion statements are: \n",
    "- It helps catching bugs early by verifying conditions at specific points because it improves code reliability by ensuring assumptions hold.\n",
    "- It makes code easier to understand by explicitly showing expected behaviors.\n",
    "- It prevents invalid inputs or states from propagating through the program.\n",
    "- During development, assertions complement testing with lightweight checks and it can be disabled in production to avoid performance issues.\n",
    "  \n",
    "Overall, they enhance code robustness and make debugging easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data \n",
    "def read_lyrics_data(lyrics_folder):\n",
    "    lyrics_data = {}\n",
    "    # Loop through each artist's subfolder\n",
    "    for artist in os.listdir(lyrics_folder):\n",
    "        artist_folder = os.path.join(lyrics_folder, artist)\n",
    "        lyrics_data[artist] = []\n",
    "        \n",
    "        # Loop through songs in artist's folder\n",
    "        for song_file in os.listdir(artist_folder):\n",
    "            song_path = os.path.join(artist_folder, song_file)\n",
    "            with open(song_path, 'r', encoding='utf-8') as f:\n",
    "                lyrics_content = f.read().strip()\n",
    "                lyrics_data[artist].append(lyrics_content)\n",
    "                # print(f\"{artist}: {lyrics_content}\")\n",
    "    return lyrics_data\n",
    "lyrics_data = read_lyrics_data(lyrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "target_filenames = ['robynkonichiwa_followers_data.txt', 'cher_followers_data.txt']\n",
    "def read_twitter_data(twitter_folder):\n",
    "    twitter_data = {}\n",
    "    for filename in os.listdir(twitter_folder):\n",
    "        if filename in target_filenames:\n",
    "            file_path = os.path.join(twitter_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                twitter_data[filename] = f.read().strip()\n",
    "    return twitter_data\n",
    "twitter_data = read_twitter_data(twitter_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb62b7-de24-42bc-ab6e-2c48f5c45310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename, description in twitter_data.items():\n",
    "#     print(f\"Filename: {filename}\")\n",
    "#     print(f\"Description:\\n{description[:1000]}\")\n",
    "#     print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "def clean_text(text):\n",
    "    # Remove punctuation include hashtag\n",
    "    # text = ''.join(char for char in text if char not in string.punctuation)\n",
    "    text = ''.join(char for char in text if char not in string.punctuation.replace('#', '')) # Thanks to ChatGPT  \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove \"lyrics'\n",
    "    text = re.sub(r'\\blyrics\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create your clean twitter data here \n",
    "# remove #\n",
    "def clean_twitter_data(twitter_data):\n",
    "    cleaned_twitter_data = {}\n",
    "    for filename, description in twitter_data.items():\n",
    "        cleaned_tokens = re.sub(r'#(\\w+)', r'\\1', description)\n",
    "        cleaned_tokens = clean_text(cleaned_tokens)\n",
    "        cleaned_twitter_data[filename] = cleaned_tokens\n",
    "    return cleaned_twitter_data\n",
    "cleaned_twitter_data = clean_twitter_data(twitter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907f2dd-60e9-421a-a160-efd66916b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename, tokens in cleaned_twitter_data.items():\n",
    "#     print(f\"Filename: {filename}\")\n",
    "#     print(f\"Cleaned Description (first 1000 characters):\\n{tokens[:1000]}\")\n",
    "#     print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean lyrics data here\n",
    "def clean_lyrics_data(lyrics_data):\n",
    "    cleaned_lyrics = {}\n",
    "    for artist, songs in lyrics_data.items():\n",
    "        cleaned_lyrics[artist] = []\n",
    "        for song_lyrics in songs:\n",
    "            # cleaned_song = re.sub(r'\\s+', ' ', song_lyrics) # addition to the last question\n",
    "            # cleaned_song = clean_text(cleaned_song)          \n",
    "            cleaned_song = clean_text(song_lyrics)          \n",
    "            cleaned_song = [word for word in cleaned_song if word != 'azlyricscom']\n",
    "            cleaned_lyrics[artist].append(cleaned_song)\n",
    "            # print(cleaned_lyrics)\n",
    "    return cleaned_lyrics\n",
    "cleaned_lyrics_data = clean_lyrics_data(lyrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f2fb7-70a2-426b-9b86-b4f07d6cf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_tokens = {}\n",
    "# Lyric Stats\n",
    "for artist, songs in cleaned_lyrics_data.items():\n",
    "    # Flatten all song tokens for the current artist\n",
    "    # Since Lyrics is in Nested Dict.\n",
    "    all_tokens = list(itertools.chain.from_iterable(songs))\n",
    "    artist_tokens[artist] = all_tokens\n",
    "\n",
    "for artist, tokens in artist_tokens.items():\n",
    "    print(f\"Artist lyric contents: {artist}\")\n",
    "    stats = descriptive_stats(tokens, num_tokens=5, verbose=True)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32013a-b41c-482c-be3c-d00229904a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter Stats\n",
    "for filename, tokens in cleaned_twitter_data.items():\n",
    "    print(f\"Filename: {filename}\")\n",
    "    stats = descriptive_stats(tokens, num_tokens=5, verbose=True)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: If we left stopwords which include conjunctions, prepositions, articles, pronouns, and auxiliary verbs, the result will generate less meaningful words, because there would be possibilities that these stopwords would be most dominant. It could skew the results since it emphasizes frequent words that don't add much insight into the textâ€™s content. On the otherhands, removing stopwords allows the analysis to get more significant and content-rich words. This often leads to a list of top words that better reflect the core themes and topics of the text. By excluding stopwords, it would enhance the relevance and accuracy of text analysis. The final result will highlight words that contribute more meaningfully to understanding the text. \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: Lexical diversity refers to the variety of unique words used in a text or speech. It reflects the richness of the vocabulary in the lyrics. I was under impression that most of artists use similar range of vocabulary in their lyrics as long as their songs express similar themes regardless the genre. After analyzing the artists' lyrics, I found that their lexical diversity was similar; which would support the belief that artists generally have a comparable vocabulary range. However, an interesting finding would suggest that factors such as genre or lyrical approach would play a more substantial role. These differences would challenge my prior assumption and highlight how artistic style impacts language or words use. For example, an artist who sings a love theme for hiphop or rap genre, the lyrics might contain more explicit languages compare to country or pop genre. If we include specific stopword based genre themes, the results confirm that lexical diversity is consistent across artists based on the themes. Thus, the analysis provides valuable insights into the relationship between lyrical content and lexical diversity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"â¤ï¸\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis ðŸ˜\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bb8da-8d1e-4405-a3ec-2e4eb0f92f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "# Thanks to ChatGPT  \n",
    "def group_emojis_by_artist(cleaned_twitter_data):\n",
    "    grouped_emojis = {}\n",
    "    for filename, tokens in cleaned_twitter_data.items():\n",
    "        artist = filename.split('_')[0]\n",
    "        text = ' '.join(tokens)\n",
    "        emojis = extract_emojis(text)\n",
    "        if artist not in grouped_emojis:\n",
    "            grouped_emojis[artist] = []\n",
    "        grouped_emojis[artist].extend(emojis)\n",
    "    return grouped_emojis\n",
    "\n",
    "grouped_emojis = group_emojis_by_artist(cleaned_twitter_data)\n",
    "most_common_emojis = {}\n",
    "\n",
    "for artist, emojis in grouped_emojis.items():\n",
    "    emojis_counter = Counter(emojis)\n",
    "    stats = descriptive_stats(emojis_counter, num_tokens=10, verbose=False)\n",
    "    most_common_emojis[artist] = stats[4]\n",
    "\n",
    "for artist, common_tokens in most_common_emojis.items():\n",
    "    print(f\"Artist: {artist}\")\n",
    "    print(\"Most common emojis in twitter:\")\n",
    "    for token, count in common_tokens:\n",
    "        print(f\"{token}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a7110-25e3-4fa4-8570-7bcedce18357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep #\n",
    "def clean_twitter_data(twitter_data):\n",
    "    cleaned_twitter_data = {}\n",
    "    for filename, description in twitter_data.items():\n",
    "        cleaned_tokens = clean_text(description)\n",
    "        cleaned_twitter_data[filename] = cleaned_tokens\n",
    "    return cleaned_twitter_data\n",
    "cleaned_twitter_data = clean_twitter_data(twitter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6a30f-c1f0-43b8-a5b3-75d319f046da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(text): \n",
    "    return re.findall(r'#\\w+', text)\n",
    "\n",
    "def combine_hashtags_back(twitter_data, cleaned_twitter_data): \n",
    "    combined_data = {}\n",
    "    for filename, description in twitter_data.items():\n",
    "        cleaned_tokens = cleaned_twitter_data[filename]\n",
    "        hashtags = extract_hashtags(description)\n",
    "        combined_tokens = cleaned_tokens + hashtags\n",
    "        combined_data[filename] = combined_tokens\n",
    "    return combined_data\n",
    "\n",
    "recombined_cleaned_twitter_data = combine_hashtags_back(twitter_data, cleaned_twitter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8da8ec-109b-416b-acb2-98655055841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_hashtags_by_artist(recombined_cleaned_twitter_data):\n",
    "    grouped_hashtags = {}\n",
    "    for filename, tokens in recombined_cleaned_twitter_data.items():\n",
    "        artist = filename.split('_')[0]\n",
    "        text = ' '.join(tokens)\n",
    "        hashtags = extract_hashtags(text)\n",
    "        if artist not in grouped_hashtags:\n",
    "            grouped_hashtags[artist] = []\n",
    "        grouped_hashtags[artist].extend(hashtags)\n",
    "    return grouped_hashtags\n",
    "\n",
    "grouped_hashtags = group_hashtags_by_artist(recombined_cleaned_twitter_data)\n",
    "most_common_hashtags = {}\n",
    "\n",
    "for artist, hashtags in grouped_hashtags.items():\n",
    "    hashtag_counter = Counter(hashtags)\n",
    "    stats = descriptive_stats(hashtag_counter, num_tokens=10, verbose=False)\n",
    "    most_common_hashtags[artist] = stats[4]\n",
    "\n",
    "for artist, common_tokens in most_common_hashtags.items():\n",
    "    print(f\"Artist: {artist}\")\n",
    "    print(\"Most common hashtag in twitter:\")\n",
    "    for token, count in common_tokens:\n",
    "        print(f\"{token}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b4257-cb1d-4193-9fd8-ec4cc72f8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_title(text):\n",
    "    match = re.search(r'- (.+?) \\|', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()\n",
    "    \n",
    "def process_lyrics_data(lyrics_data):\n",
    "    song_titles = {}\n",
    "    for artist, lyrics_list in lyrics_data.items():\n",
    "        song_titles[artist] = []\n",
    "        for lyrics in lyrics_list:\n",
    "            lines = lyrics.split('\\n')\n",
    "            if lines:\n",
    "                song_title = lines[0].strip()\n",
    "                song_title = song_title.replace(' Lyrics', '')\n",
    "                extracted_title = extract_song_title(song_title)\n",
    "                cleaned_title = clean_text(extracted_title)\n",
    "                song_titles[artist].extend(cleaned_title)\n",
    "    return song_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "song_titles = process_lyrics_data(lyrics_data)\n",
    "most_common_tokens_by_artist_title = {}\n",
    "\n",
    "for artist, tokens in song_titles.items():\n",
    "    counter = Counter(tokens)\n",
    "    stats = descriptive_stats(counter, num_tokens=5, verbose=False)\n",
    "    most_common_tokens_by_artist_title[artist] = stats[4]\n",
    "\n",
    "for artist, common_tokens in most_common_tokens_by_artist_title.items():\n",
    "    print(f\"Artist: {artist}\")\n",
    "    print(\"Most common tokens in song titles:\")\n",
    "    for token, count in common_tokens:\n",
    "        print(f\"{token}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: The regular expression '\\s+' matches one or more consecutive whitespace characters such as spaces, tabs, newlines. \n",
    "This expression is usually used for normalizing text on removing whitespace(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9f24a-d028-4786-8146-62ee81c91906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all songs length by artist\n",
    "data = []\n",
    "for artist, songs in cleaned_lyrics_data.items():\n",
    "    for i, song_tokens in enumerate(songs):\n",
    "        song_length = len(tokenize_lyrics(' '.join(song_tokens)))\n",
    "        data.append({'artist': artist, 'song': f'song{i+1}', 'length': song_length})        \n",
    "           \n",
    "df = pd.DataFrame(data)\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "# By lyric \n",
    "lyric_lengths = {}\n",
    "for artist, songs in cleaned_lyrics_data.items():\n",
    "    lyric_lengths[artist] = [len(tokenize_lyrics(' '.join(song))) for song in songs]\n",
    "\n",
    "songs = []\n",
    "lengths = []\n",
    "for artist, lengths_list in lyric_lengths.items():\n",
    "    for i, length in enumerate(lengths_list): # Flatten the Nested Dic\n",
    "        songs.append(f\"{artist}_song{i+1}\")\n",
    "        lengths.append(length)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(songs, lengths, color='lightblue')\n",
    "plt.title('Lyric Length By Songs')\n",
    "plt.xlabel('Song#')\n",
    "plt.ylabel('Number of Words in a song')\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf20508-8415-49a3-b54a-bcc0963024ff",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "- Oracle and/or its affiliates. (n.d.). Programming With Assertions. Programming with assertions. https://docs.oracle.com/javase/8/docs/technotes/guides/language/assert.html#:~:text=An%20assertion%20is%20a%20statement,than%20the%20speed%20of%20light.\n",
    "- Wikimedia Foundation. (2024, August 14). Lexical diversity. Wikipedia. https://en.wikipedia.org/wiki/Lexical_diversity\n",
    "- OpenAI. (2024). ChatGPT (September 24 version) [Large language model]. https://chat.openai.com/chat\n",
    "- Dib, F. (n.d.). Build, test, and debug regex. regex101. https://regex101.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b470b35-7012-4198-88d8-55929633c8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
